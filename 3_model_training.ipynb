{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> # Mestrado Engenharia InformÃ¡tica\n",
    ">\n",
    "> ## **Algoritmos para Big Data**\n",
    "\n",
    "> ### **Felipe Silva** # 121851\n",
    "\n",
    "> ### **Fernando Piedade** # 109266\n",
    "\n",
    "> **_*2023/24*_**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Model Training**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initial settings\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional packages and imports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark, pyspark\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "import pyspark.sql.functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /home/felipesilva/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "nltk.download(\"wordnet\")\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Lemmatization reduces the words to their root or base forms, known as lemma. It was performed by WordNetLemmatizer() from the nltk.stem module. The lemmatizer called the lemmatize() function on each token present in the text and then combined the results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline, Transformer\n",
    "from pyspark.ml.feature import RegexTokenizer # tokenizer\n",
    "from pyspark.ml.feature import HashingTF, IDF # vectorizer\n",
    "from pyspark.ml.feature import StopWordsRemover # to remove stop words\n",
    "from pyspark.ml.classification import LogisticRegression, LinearSVC, OneVsRest, NaiveBayes # ml models\n",
    "from pyspark.ml.util import DefaultParamsReadable, DefaultParamsWritable\n",
    "from pyspark.ml.param.shared import HasInputCol, HasOutputCol\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator # to evaluate the model\n",
    "from pyspark.mllib.evaluation import MulticlassMetrics # # performance metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/06/02 23:26:32 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "# Create the Spark session\n",
    "\n",
    "findspark.init()\n",
    "findspark.find()\n",
    "\n",
    "spark = SparkSession\\\n",
    "        .builder\\\n",
    "        .appName(\"ADBFinacialReportsSecModelTraining\")\\\n",
    "        .config(\"spark.sql.shuffle.partitions\",100)\\\n",
    "        .config(\"spark.driver.memory\", \"16G\")\\\n",
    "        .config(\"spark.sql.repl.eagereval.enabled\",True)\\\n",
    "        .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>pre { white-space: pre !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import HTML\n",
    "display(HTML(\"<style>pre { white-space: pre !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collect data from local directory\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"../Datasets/financial-reports-sec/parquet/large/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Useful functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_if_exists(path):\n",
    "  if os.path.exists(path):\n",
    "    shutil.rmtree(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply Lemmatizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Lemmatizer(Transformer, HasInputCol, HasOutputCol, DefaultParamsReadable, DefaultParamsWritable):\n",
    "  def __init__(self, inputCol=None, outputCol=None):\n",
    "    super(Lemmatizer, self).__init__()\n",
    "    self.lemmatizer = WordNetLemmatizer()\n",
    "    self._setDefault(inputCol=inputCol, outputCol=outputCol)\n",
    "    self._set(inputCol=inputCol, outputCol=outputCol)\n",
    "  \n",
    "  def _transform(self, df):\n",
    "    lemmatize_udf = F.udf(lambda words: [self.lemmatizer.lemmatize(word) for word in words], ArrayType(StringType()))\n",
    "    return df.withColumn(self.getOutputCol(), lemmatize_udf(df[self.getInputCol()]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read parquet files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = spark.read.parquet(data_dir + \"test.parquet\")\n",
    "df_train = spark.read.parquet(data_dir + \"train.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert sentences to list of words\n",
    "tokenizer = RegexTokenizer(inputCol=\"report\", outputCol=\"words\", pattern=\"\\\\W\")\n",
    "\n",
    "# to remove stop words like is, the, in, etc.\n",
    "stopwords_remover = StopWordsRemover(inputCol=\"words\", outputCol=\"filtered\")\n",
    "\n",
    "# reduces the words to their root or base forms, known as lemma.\n",
    "lemmatizer = Lemmatizer(inputCol=\"filtered\", outputCol=\"lemmatized\")\n",
    "\n",
    "# Calculate term frequency in each article\n",
    "hashing_tf = HashingTF(inputCol=\"filtered\", outputCol=\"raw_features\",  numFeatures=50000)\n",
    "hashing_tf_with_lemmatizer = HashingTF(inputCol=\"lemmatized\", outputCol=\"raw_features\",  numFeatures=50000)\n",
    "\n",
    "# Inverse document frequency\n",
    "idf = IDF(inputCol=\"raw_features\", outputCol=\"features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification Models\n",
    "\n",
    "### Logistic Regression\n",
    "\n",
    "### Linear SVC\n",
    "\n",
    "### Naive Bayes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model object\n",
    "lr = LogisticRegression(regParam=0.3, maxIter=50)\n",
    "ovr_lr = OneVsRest(classifier=lr)\n",
    "\n",
    "lsvc = LinearSVC(maxIter=50, regParam=0.3)\n",
    "ovr_lsvc = OneVsRest(classifier=lsvc)\n",
    "\n",
    "nb = NaiveBayes()\n",
    "ovr_nb = OneVsRest(classifier=nb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_lr = Pipeline(stages=[tokenizer, stopwords_remover, hashing_tf, idf, ovr_lr])\n",
    "pipeline_lsvc = Pipeline(stages=[tokenizer, stopwords_remover, hashing_tf, idf, ovr_lsvc])\n",
    "pipeline_nb = Pipeline(stages=[tokenizer, stopwords_remover, hashing_tf, idf, ovr_nb])\n",
    "\n",
    "pipeline_lr_lemmatizer = Pipeline(stages=[tokenizer, stopwords_remover, lemmatizer, hashing_tf_with_lemmatizer, idf, lr])\n",
    "pipeline_lsvc_lemmatizer = Pipeline(stages=[tokenizer, stopwords_remover, lemmatizer, hashing_tf_with_lemmatizer, idf, ovr_lsvc])\n",
    "pipeline_nb_lemmatizer = Pipeline(stages=[tokenizer, stopwords_remover, lemmatizer, hashing_tf_with_lemmatizer, idf, ovr_nb])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipelines = [\n",
    "  { \"name\": \"LogisticRegression\", \"pipeline\": pipeline_lr },\n",
    "  { \"name\": \"LinearSVC\", \"pipeline\": pipeline_lsvc },\n",
    "  { \"name\": \"NaiveBayes\", \"pipeline\": pipeline_nb },\n",
    "  { \"name\": \"LogisticRegression_Lemmatized\", \"pipeline\": pipeline_lr_lemmatizer },\n",
    "  { \"name\": \"LinearSVC_Lemmatized\", \"pipeline\": pipeline_lsvc_lemmatizer },\n",
    "  { \"name\": \"NaiveBayes_Lemmatized\", \"pipeline\": pipeline_nb_lemmatizer }\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Pipeline Models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LinearSVC\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NaiveBayes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression_Lemmatized\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/06/03 00:01:36 WARN TaskSetManager: Stage 2888 contains a task of very large size (3197 KiB). The maximum recommended task size is 1000 KiB.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LinearSVC_Lemmatized\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NaiveBayes_Lemmatized\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "for el in pipelines:\n",
    "  name = el[\"name\"]\n",
    "  pipeline = el[\"pipeline\"]\n",
    "\n",
    "  print(name)\n",
    "\n",
    "  path = f\"pipelines/{name}\"\n",
    "  remove_if_exists(path)\n",
    "  pipeline.save(path)\n",
    "\n",
    "  pipeline_model = pipeline.fit(df_train)\n",
    "\n",
    "  path = f\"pipelines_model/{name}\"\n",
    "  remove_if_exists(path)\n",
    "  pipeline_model.save(path)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyspark_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
