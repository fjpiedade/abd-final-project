{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark, pyspark\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "import pyspark.sql.functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/05/19 20:11:59 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "# Create the Spark session\n",
    "\n",
    "findspark.init()\n",
    "findspark.find()\n",
    "\n",
    "spark = SparkSession\\\n",
    "        .builder\\\n",
    "        .appName(\"HomeCreditDefaultRisk\")\\\n",
    "        .config(\"spark.sql.shuffle.partitions\",6)\\\n",
    "        .config(\"spark.driver.memory\", \"12G\")\\\n",
    "        .config(\"spark.sql.repl.eagereval.enabled\",True)\\\n",
    "        .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>pre { white-space: pre !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import HTML\n",
    "display(HTML(\"<style>pre { white-space: pre !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"../Datasets/financial-reports-sec/data/\"\n",
    "output_dir = \"../Datasets/financial-reports-sec/parquet/\"\n",
    "data_sizes = [\"small\", \"large\"]\n",
    "data_types = [\"test\", \"train\", \"validate\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of sections to concatenate\n",
    "sections = [\"filing.report.section_1\", \"filing.report.section_1A\", \"filing.report.section_1B\",\n",
    "            \"filing.report.section_2\", \"filing.report.section_3\", \"filing.report.section_4\",\n",
    "            \"filing.report.section_5\", \"filing.report.section_6\", \"filing.report.section_7\",\n",
    "            \"filing.report.section_7A\", \"filing.report.section_8\", \"filing.report.section_9\",\n",
    "            \"filing.report.section_9A\", \"filing.report.section_9B\", \"filing.report.section_10\",\n",
    "            \"filing.report.section_11\", \"filing.report.section_12\", \"filing.report.section_13\",\n",
    "            \"filing.report.section_14\", \"filing.report.section_15\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concat_sections(col_list):\n",
    "  # Concatenate elements in each array into a single string for each section\n",
    "  concat_array_cols = [F.concat_ws(\" \", F.col(section)).alias(section) for section in col_list]\n",
    "  \n",
    "  return F.concat_ws(\" \", *concat_array_cols).alias(\"full_text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_transform(df):\n",
    "  # Explode the filings array to get one row per filing\n",
    "  df_exploded = df.withColumn(\"filing\", F.explode(\"filings\")).drop(\"filings\")\n",
    "\n",
    "  # Concatenate all sections into one single text field\n",
    "  df_exploded = df_exploded.withColumn(\"full_text\", concat_sections(sections))\n",
    "\n",
    "  # Extract labels, dates and returns fields\n",
    "  df_exploded = df_exploded \\\n",
    "  .withColumn(\"labels_1d\", F.col(\"filing.labels.1d\")) \\\n",
    "  .withColumn(\"labels_30d\", F.col(\"filing.labels.30d\")) \\\n",
    "  .withColumn(\"labels_5d\", F.col(\"filing.labels.5d\")) \\\n",
    "  .withColumn(\"returns_1d_closePriceEndDate\", F.col(\"filing.returns.1d.closePriceEndDate\")) \\\n",
    "  .withColumn(\"returns_1d_closePriceStartDate\", F.col(\"filing.returns.1d.closePriceStartDate\")) \\\n",
    "  .withColumn(\"returns_1d_endDate\", F.col(\"filing.returns.1d.endDate\")) \\\n",
    "  .withColumn(\"returns_1d_ret\", F.col(\"filing.returns.1d.ret\")) \\\n",
    "  .withColumn(\"returns_1d_startDate\", F.col(\"filing.returns.1d.startDate\")) \\\n",
    "  .withColumn(\"returns_30d_closePriceEndDate\", F.col(\"filing.returns.30d.closePriceEndDate\")) \\\n",
    "  .withColumn(\"returns_30d_closePriceStartDate\", F.col(\"filing.returns.30d.closePriceStartDate\")) \\\n",
    "  .withColumn(\"returns_30d_endDate\", F.col(\"filing.returns.30d.endDate\")) \\\n",
    "  .withColumn(\"returns_30d_ret\", F.col(\"filing.returns.30d.ret\")) \\\n",
    "  .withColumn(\"returns_30d_startDate\", F.col(\"filing.returns.30d.startDate\")) \\\n",
    "  .withColumn(\"returns_5d_closePriceEndDate\", F.col(\"filing.returns.5d.closePriceEndDate\")) \\\n",
    "  .withColumn(\"returns_5d_closePriceStartDate\", F.col(\"filing.returns.5d.closePriceStartDate\")) \\\n",
    "  .withColumn(\"returns_5d_endDate\", F.col(\"filing.returns.5d.endDate\")) \\\n",
    "  .withColumn(\"returns_5d_ret\", F.col(\"filing.returns.5d.ret\")) \\\n",
    "  .withColumn(\"returns_5d_startDate\", F.col(\"filing.returns.5d.startDate\")) \\\n",
    "  .withColumn(\"acceptanceDateTime\", F.col(\"filing.acceptanceDateTime\")) \\\n",
    "  .withColumn(\"filingDate\", F.col(\"filing.filingDate\")) \\\n",
    "  .withColumn(\"reportDate\", F.col(\"filing.reportDate\")) \\\n",
    "  .withColumn(\"form\", F.col(\"filing.form\")) \\\n",
    "  .drop(\"filing\")\n",
    "\n",
    "  return df_exploded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "small.test\n",
      "+----------+----------------+\n",
      "|cik       |full_text_length|\n",
      "+----------+----------------+\n",
      "|0001602658|387409          |\n",
      "|0001602658|430892          |\n",
      "|0001602658|436873          |\n",
      "|0001602658|400906          |\n",
      "|0001602658|423870          |\n",
      "|0001602658|363616          |\n",
      "|0001602658|370596          |\n",
      "|0001603145|268967          |\n",
      "|0001603145|300016          |\n",
      "|0001603145|91466           |\n",
      "|0001602813|44500           |\n",
      "|0001602813|49774           |\n",
      "|0001602813|51027           |\n",
      "|0001602813|60283           |\n",
      "+----------+----------------+\n",
      "\n",
      "14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/19 20:12:06 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "small.train\n",
      "+----------+----------------+\n",
      "|cik       |full_text_length|\n",
      "+----------+----------------+\n",
      "|0000002488|264393          |\n",
      "|0000002488|164104          |\n",
      "|0000002488|308715          |\n",
      "|0000002488|367277          |\n",
      "|0000002488|350526          |\n",
      "|0000002488|323278          |\n",
      "|0000002488|311993          |\n",
      "|0000002488|294435          |\n",
      "|0000002488|297597          |\n",
      "|0000002488|338345          |\n",
      "|0000002488|379762          |\n",
      "|0000002488|396306          |\n",
      "|0000002488|492154          |\n",
      "|0000002488|540034          |\n",
      "|0000002488|538078          |\n",
      "|0000002488|440732          |\n",
      "|0000002488|416625          |\n",
      "|0000002488|304806          |\n",
      "|0000002488|250897          |\n",
      "|0000002488|71367           |\n",
      "+----------+----------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "188\n",
      "small.validate\n",
      "+----------+----------------+\n",
      "|cik       |full_text_length|\n",
      "+----------+----------------+\n",
      "|0001699150|399171          |\n",
      "|0001699150|366235          |\n",
      "|0001699150|375938          |\n",
      "|0001699150|307374          |\n",
      "|0001699136|334676          |\n",
      "|0001699136|335966          |\n",
      "|0001699136|351644          |\n",
      "|0001699136|262459          |\n",
      "|0001699039|293321          |\n",
      "|0001699039|280668          |\n",
      "|0001699039|337555          |\n",
      "|0001699039|357800          |\n",
      "+----------+----------------+\n",
      "\n",
      "12\n",
      "large.test\n",
      "+----------+----------------+\n",
      "|cik       |full_text_length|\n",
      "+----------+----------------+\n",
      "|0001603978|195588          |\n",
      "|0001603978|177600          |\n",
      "|0001603978|201084          |\n",
      "|0001603978|204737          |\n",
      "|0001603978|193901          |\n",
      "|0001604868|187379          |\n",
      "|0001604868|142094          |\n",
      "|0001604868|192173          |\n",
      "|0001604868|188269          |\n",
      "|0001606498|178227          |\n",
      "|0001606498|231735          |\n",
      "|0001606498|239580          |\n",
      "|0001606498|243069          |\n",
      "|0001606498|235619          |\n",
      "|0001606498|230327          |\n",
      "|0001606498|208136          |\n",
      "|0001609139|276158          |\n",
      "|0001609139|207981          |\n",
      "|0001609139|173618          |\n",
      "|0001610853|416313          |\n",
      "+----------+----------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "1820\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "large.train\n",
      "+----------+----------------+\n",
      "|cik       |full_text_length|\n",
      "+----------+----------------+\n",
      "|0000001961|113500          |\n",
      "|0000001961|105416          |\n",
      "|0000001961|103792          |\n",
      "|0000001961|96718           |\n",
      "|0000001961|105987          |\n",
      "|0000001961|68409           |\n",
      "|0000001961|66589           |\n",
      "|0000001961|64647           |\n",
      "|0000001961|56478           |\n",
      "|0000001961|222507          |\n",
      "|0000001961|105681          |\n",
      "|0000001961|104978          |\n",
      "|0000001961|79863           |\n",
      "|0000003570|317495          |\n",
      "|0000003570|350513          |\n",
      "|0000003570|330979          |\n",
      "|0000003570|338654          |\n",
      "|0000003570|336623          |\n",
      "|0000003570|311212          |\n",
      "|0000003570|317898          |\n",
      "+----------+----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "52663\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "large.validate\n",
      "+----------+----------------+\n",
      "|cik       |full_text_length|\n",
      "+----------+----------------+\n",
      "|0001699150|399171          |\n",
      "|0001699150|366235          |\n",
      "|0001699150|375938          |\n",
      "|0001699150|500846          |\n",
      "|0001701732|386310          |\n",
      "|0001701732|406218          |\n",
      "|0001701732|407962          |\n",
      "|0001701732|359322          |\n",
      "|0001703073|114735          |\n",
      "|0001703073|74832           |\n",
      "|0001705682|356882          |\n",
      "|0001705682|349997          |\n",
      "|0001705682|323060          |\n",
      "|0001705682|343486          |\n",
      "|0001707925|228973          |\n",
      "|0001707925|294159          |\n",
      "|0001707925|287521          |\n",
      "|0001707925|104090          |\n",
      "|0001709164|191527          |\n",
      "|0001709164|168253          |\n",
      "+----------+----------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "866\n"
     ]
    }
   ],
   "source": [
    "for size in data_sizes:\n",
    "  for type in data_types:\n",
    "    filename = data_dir + size + \"/\" + type\n",
    "    output = output_dir + size + \"/\" + type + \".parquet\"\n",
    "    df = spark.read.json(filename)\n",
    "    df = df_transform(df)\n",
    "    df = df.withColumn(\"full_text_length\", F.length(F.col(\"full_text\")))\n",
    "    print(size + \".\" + type)\n",
    "    df.select(\"cik\", \"full_text_length\").show(truncate=False)\n",
    "    df = df.drop(\"full_text_length\")\n",
    "    print(df.count())\n",
    "    df.write.mode(\"overwrite\").parquet(output)\n",
    "\n",
    "# df.write.format(\"json\").mode(\"overwrite\").save(\"tmp/json_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyspark_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
