{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ## **Algoritmos para Big Data**\n",
    "\n",
    "> ### **Filipe Silva**\n",
    "\n",
    "> ### **Fernando Piedade**\n",
    "\n",
    "> **_*2023/24*_**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Introduction\n",
    "\n",
    "This project aims to consolidate practical knowledge in the design and implementation of a computational solution to respond to a large-scale data analysis problem.\n",
    "\n",
    "In terms of implementation tools, the project must essentially use features provided by the Apache Spark platform and the Python programming language.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Structured streaming\n",
    "\n",
    "A key aspect of structured streaming is to acquire/send data from a streaming data producer/consumer. That is, from a streaming source/sink.\n",
    "\n",
    "Apache Spark provides methods to read/write from/to a stream,\n",
    "accordingly to some formats we may select from. Of course, some kind of configuration is required.\n",
    "\n",
    "Firstly, there are the usual file-based formats like json, parquet, csv, text, and so on.\n",
    "Also, we can use socket connections to get/send text data from/to TCP servers, and more importantly, we can rely on functionalities of advanced message systems like Apache Kafka, which will play a sort of buffering role.\n",
    "\n",
    "Secondly, we have to set an output mode, which defines how the results will be delivered. For instance, to see all data every time, only updates, or just the new records.\n",
    "\n",
    "Further details can be found in https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Problem formulation\n",
    "\n",
    "This exercise is about detecting credit card frauds in near real-time. This case-study is based on a Kaggle dataset that it is available in https://www.kaggle.com/datasets/ealtman2019/credit-card-transactions. The dataset is expected to be a realistic example of synthetic data regarding credit card transactions.\n",
    "\n",
    "We assume that a ML classification model has already been created and made available. Therefore, now we have to deal with a stream of transactions that are expected to be processed, like it would be in in a real-time scenario. Hence, we will simulate such scenario, mostly relying on Spark's Structured Streaming.\n",
    "\n",
    "The functional requirements for the Spark program we are going to create are as follows:\n",
    "\n",
    "1. To load a ML model previously built.\n",
    "2. To process credit card transactions held in a simulated data stream, by applying the ML model.\n",
    "3. To explore the results obtained.\n",
    "\n",
    "Also, in order to speed up some processing, we will use some files that were computed in advance.\n",
    "\n",
    "**Data available**\n",
    "\n",
    "- The dataset about the stream of credit card transactions can be downloaded from the location\n",
    "\n",
    "> https://bigdata.iscte.me/abd/credit-card-transactions-stream.zip\n",
    "\n",
    "- The ML classification model already built can be downloaded from the location\n",
    "\n",
    "> https://bigdata.iscte.me/abd/model-LinearSVM-credit-cards.zip\n",
    "\n",
    "All the files are in parquet format.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initial settings\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional packages and imports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark, pyspark\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "import pyspark.sql.functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-07T19:11:08.158170Z",
     "start_time": "2021-03-07T19:11:07.859222Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import os, sys \n",
    "import time\n",
    "\n",
    "import numpy as np \n",
    "import pandas as pd  \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/05/14 00:09:33 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "# Create the Spark session\n",
    "\n",
    "findspark.init()\n",
    "findspark.find()\n",
    "\n",
    "spark = SparkSession\\\n",
    "        .builder\\\n",
    "        .appName(\"StreamingCreditCards\")\\\n",
    "        .config(\"spark.sql.shuffle.partitions\",6)\\\n",
    "        .config(\"spark.sql.repl.eagereval.enabled\",True)\\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://fernandos-mbp.lan:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>StreamingCreditCards</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x11f4a3ed0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some Spark related imports we will use hereafter\n",
    "\n",
    "from pyspark.ml import PipelineModel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>pre { white-space: pre !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import HTML\n",
    "display(HTML(\"<style>pre { white-space: pre !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Useful functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotBarColoured(df, xcol, ycol, colour): \n",
    "    return sns.barplot(data=df, x=xcol, y=ycol, color=colour)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "# Collect and label data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-07T19:11:08.953150Z",
     "start_time": "2021-03-07T19:11:08.185863Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# ! pwd & ls -la\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Checking working directory and data files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/fernandopiedade/big-data/abd-final-project'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = '/Users/fernandopiedade/big-data/abd-final-project'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 208\n",
      "drwxr-xr-x   8 fernandopiedade  staff    256 May 13 20:01 \u001b[34m.\u001b[m\u001b[m/\n",
      "drwxr-xr-x   7 fernandopiedade  staff    224 May  2 12:48 \u001b[34m..\u001b[m\u001b[m/\n",
      "-rw-r--r--@  1 fernandopiedade  staff   6148 May 13 20:01 .DS_Store\n",
      "drwxr-xr-x  13 fernandopiedade  staff    416 Apr 25 10:10 \u001b[34m.git\u001b[m\u001b[m/\n",
      "-rw-r--r--   1 fernandopiedade  staff    121 May 13 20:01 .gitignore\n",
      "-rw-r--r--   1 fernandopiedade  staff    210 May  9 22:21 README.md\n",
      "-rw-rw-r--   1 fernandopiedade  staff  88503 May 14 00:00 abd-final-project.ipynb\n",
      "drwxr-xr-x   5 fernandopiedade  staff    160 May 13 21:44 \u001b[34mdatasets\u001b[m\u001b[m/\n"
     ]
    }
   ],
   "source": [
    "ls -la $data_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading the dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-07T19:11:12.476668Z",
     "start_time": "2021-03-07T19:11:08.962435Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Reading data\n",
    "\n",
    "# Stackoverflow\n",
    "#data_dir = data_path + '/datasets/stackoverflow-questions/data/post_questions_test/'\n",
    "\n",
    "file_path = data_path + '/datasets/stackoverflow-questions/data/post_questions_test/'\n",
    "\n",
    "df_stackoverflow = spark.read.parquet(file_path)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking data\n",
    "\n",
    "Schema, show and count.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- title: string (nullable = true)\n",
      " |-- body: string (nullable = true)\n",
      " |-- label: long (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----+\n",
      "|title                                                                    |body                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             |label|\n",
      "+-------------------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----+\n",
      "|How to read FormData into WebAPI                                         |<p>I have an ASP.NET MVC WebApplication where I am using the ASP.NET Web API framework.</p>\\n\\n<p><strong>Javascript code:</strong></p>\\n\\n<pre><code>var data = new FormData();\\ndata.append(\"filesToDelete\", \"Value\");\\n\\n$.ajax({    \\n    type: \"POST\",\\n    url: \"/api/FileAttachment/UploadFiles?clientContactId=\" + clientContactId,\\n    contentType: false,\\n    processData: false,\\n    data: data,\\n    success: function (result) {\\n        // Do something\\n    },\\n    error: function (xhr, status, p3, p4) {\\n        // Do something\\n    }\\n});\\n</code></pre>\\n\\n<p><strong>C# code (WebAPI):</strong></p>\\n\\n<pre><code>public void UploadFiles(int clientContactId) {\\n    if (!Request.Content.IsMimeMultipartContent()) {\\n        throw new HttpResponseException(HttpStatusCode.UnsupportedMediaType);\\n    }\\n\\n    var jsonContent = Request.Content.ReadAsStringAsync().Result;\\n}\\n</code></pre>\\n\\n<p>How do I read <code>jsonContent</code> based on a key value pair passed by the Javascript FormData? </p>\\n\\n<p>I tried to do <code>JsonConvert.DeserializeObject&lt;?&gt;</code>, but it requires a particular type to deserialize into.</p>\\n\\n<p>I want to get the value of the key <code>\"filesToDelete\"</code> passed from the Javascript FormData.</p>\\n\\n<p>How can I get this value?</p>                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            |0    |\n",
      "|vertical-align: middle not working                                       |<p>I have a section title which has a border on both left and right sides. Ideally, I'd like these to be vertically aligned. However, <code>vertical-align: middle;</code> isn't working and I'm not too sure why. Ideas?</p>\\n\\n<p>CSS</p>\\n\\n<pre><code>#save-the-date {\\n    margin: 0 auto;\\n    width: 960px;\\n}\\n\\n#save-the-date #title {\\n    vertical-align: middle;\\n}\\n\\n#save-the-date #title h2 {\\n    font-size: 165%;\\n    margin: 0 auto;\\n    width: 150px;\\n}\\n\\n#save-the-date #title .left-border {\\n    float: left;\\n    background:url('img/border.png') repeat-x -10px 0;\\n    width:340px;\\n    height:3px;\\n}\\n\\n#save-the-date #title .right-border {\\n    float: right;\\n    background:url('img/border.png') repeat-x -10px 0;\\n    width:340px;\\n    height:3px;\\n}\\n</code></pre>\\n\\n<p>HTML</p>\\n\\n<pre><code>&lt;div id=\"save-the-date\"&gt;\\n    &lt;div id=\"title\"&gt;\\n        &lt;div class=\"left-border\"&gt;&lt;/div&gt;&lt;!-- end border --&gt;\\n        &lt;h2&gt;Save The Date&lt;/h2&gt;\\n        &lt;div class=\"right-border\"&gt;&lt;/div&gt;&lt;!-- end border --&gt;\\n    &lt;/div&gt;&lt;!-- end title --&gt;\\n&lt;/div&gt;&lt;!-- end save-the-date --&gt;\\n</code></pre>\\n\\n<p><img src=\"https://i.stack.imgur.com/9MgdF.png\" alt=\"enter image description here\"></p>                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            |0    |\n",
      "|Symfony2: Database schema and entities not updating with changed metadata|<p>I am having trouble updating the schema in Symfony2.</p>\\n\\n<p>I have imported a database into Symfony2 using doctrine and that created all the ORM files in YML.</p>\\n\\n<p>I have created all the entities from this metadata and it worked great, but if I want to change the database schema using the orm.yml files, It does not update my database or even update the entities when I regenerate them.</p>\\n\\n<p>The import created the orm.yml files in \\n/src/{name}/{my}bundle/Resources/config/doctrine/metadata/orm/{table}.orm.yml</p>\\n\\n<p>It was created with no errors.</p>\\n\\n<p>When I do:</p>\\n\\n<pre><code>php app/console doctrine:schema:update --dump-sql\\n</code></pre>\\n\\n<p>it displays:</p>\\n\\n<pre><code>ALTER TABLE accounttypes CHANGE description description VARCHAR(45) NOT NULL\\n</code></pre>\\n\\n<p>So I:</p>\\n\\n<pre><code>php app/console doctrine:schema:update --force\\n</code></pre>\\n\\n<p>And:</p>\\n\\n<pre><code>Updating database schema...\\nDatabase schema updated successfully! \"1\" queries were executed\\n</code></pre>\\n\\n<p>I can do this over and over again and the same query appears and I execute it and it tells me it is done, but again it shows that it needs to be done again.</p>\\n\\n<p>I then have gone to the orm.yml files and changed them drastically, but nothing no new queries appear other than the one that is always there when I do this.</p>\\n\\n<p>The file AccountTypes.orm.yml</p>\\n\\n<pre><code>Accounttypes:\\n  type: entity\\n  table: accounttypes\\n  fields:\\n    description:\\n      id: true\\n      type: string\\n      length: 45\\n      fixed: false\\n      nullable: false\\n      generator:\\n        strategy: IDENTITY\\n  lifecycleCallbacks: {  }\\n</code></pre>\\n\\n<p>Change to this:</p>\\n\\n<pre><code>Accounttypes:\\n  type: entity\\n  table: accounttypes\\n  id:\\n    id:\\n      type: integer\\n      generator: { strategy: AUTO }\\n  fields:\\n    description:\\n      id: true\\n      type: string\\n      length: 50\\n  lifecycleCallbacks: {  }\\n</code></pre>\\n\\n<p>And it still tells me I need to:</p>\\n\\n<pre><code>ALTER TABLE accounttypes CHANGE description description VARCHAR(45) NOT NULL\\n</code></pre>\\n\\n<p>I have also tried using DoctrineMigrationsBundle and the same query shows up needing done. I migrate; It says the query is done and when I use it again the same query shows up.</p>\\n\\n<p>Does anybody have any idea how this is happening? I am stuck on this, so any help is greatly appreciated.</p>|0    |\n",
      "|How to handle error response with Retrofit 2?                            |<p>How to handle error response with Retrofit 2 using <strong>synchronous</strong> request?</p>\\n\\n<p>I need process response that in normal case return pets array and if request has bad parametrs return error json object. How can I process this two situations?</p>\\n\\n<p>I am trying to use <a href=\"https://futurestud.io/tutorials/retrofit-2-simple-error-handling\" rel=\"noreferrer\">this</a> tutorial but the main problem is mapping normal and error json to objects. </p>\\n\\n<p>My <strong>normal response</strong> example:</p>\\n\\n<pre><code>[ {\\n    \"type\" : \"cat\",\\n    \"color\": \"black\"\\n}, \\n{\\n    \"type\" : \"cat\",\\n    \"color\": \"white\"\\n} ]\\n</code></pre>\\n\\n<p><strong>Error response</strong> example:</p>\\n\\n<pre><code>{\"error\" = \"-1\", error_description = \"Low version\"}\\n</code></pre>\\n\\n<p>What I got:</p>\\n\\n<pre><code>    Call&lt;List&lt;Pet&gt;&gt; call = getApiService().getPet(1);\\n    Response&lt;List&lt;Pet&gt;&gt; response;\\n    List&lt;Pet&gt; result = null;\\n\\n    try {\\n        response = call.execute(); //line with exception \"Expected BEGIN_ARRAY but was BEGIN_OBJECT at line 1 column 2 path\"\\n        if(!response.isSuccessful()){\\n            Error error = parseError(response);\\n            Log.d(\"error message\", error.getErrorDescription());\\n        }\\n        if (response.code() == 200) {\\n            result = response.body();\\n        }\\n    } catch (IOException e) {\\n        e.printStackTrace();\\n    }\\n</code></pre>                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      |0    |\n",
      "|.NET Core SDK versions - which to uninstall?                             |<p>I have the following versions of .NET Core SDKs installed on my machine:</p>\\n\\n<p><a href=\"https://i.stack.imgur.com/kPQYJ.png\" rel=\"noreferrer\"><img src=\"https://i.stack.imgur.com/kPQYJ.png\" alt=\"enter image description here\"></a>\\n<a href=\"https://i.stack.imgur.com/BxJn0.png\" rel=\"noreferrer\"><img src=\"https://i.stack.imgur.com/BxJn0.png\" alt=\"enter image description here\"></a></p>\\n\\n<p>Please confirm that I understand what each of these is (and if I can uninstall them):</p>\\n\\n<p><strong>.NET Core SDK 1.0.0 (x64) Installer (x64)</strong>: This was installed along with VS2017</p>\\n\\n<p><strong>.NET Core SDK 1.0.1 (x64)</strong>: Downloaded somewhere <a href=\"https://www.microsoft.com/net/core\" rel=\"noreferrer\">here</a> and installed manually. Exactly the same as the 1.0.0 SDK above except that it <a href=\"https://github.com/dotnet/core/blob/master/release-notes/1.1/1.1.1.md\" rel=\"noreferrer\">includes support for Fedora 24 and OpenSUSE 42.1</a>. <strong>So as a Windows user, can I uninstall this?</strong></p>\\n\\n<p>The other four <strong>Microsoft .NET Core 1.x.x SDKs</strong> are various versions of the VS2015 (and project.json) preview tooling and can thus be uninstalled?</p>                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               |0    |\n",
      "+-------------------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1570866"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "df_stackoverflow.printSchema()\n",
    "df_stackoverflow.show(5, truncate=False)\n",
    "num_transactions = df_stackoverflow.count()\n",
    "\n",
    "num_transactions\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(title='How to read FormData into WebAPI', body='<p>I have an ASP.NET MVC WebApplication where I am using the ASP.NET Web API framework.</p>\\n\\n<p><strong>Javascript code:</strong></p>\\n\\n<pre><code>var data = new FormData();\\ndata.append(\"filesToDelete\", \"Value\");\\n\\n$.ajax({    \\n    type: \"POST\",\\n    url: \"/api/FileAttachment/UploadFiles?clientContactId=\" + clientContactId,\\n    contentType: false,\\n    processData: false,\\n    data: data,\\n    success: function (result) {\\n        // Do something\\n    },\\n    error: function (xhr, status, p3, p4) {\\n        // Do something\\n    }\\n});\\n</code></pre>\\n\\n<p><strong>C# code (WebAPI):</strong></p>\\n\\n<pre><code>public void UploadFiles(int clientContactId) {\\n    if (!Request.Content.IsMimeMultipartContent()) {\\n        throw new HttpResponseException(HttpStatusCode.UnsupportedMediaType);\\n    }\\n\\n    var jsonContent = Request.Content.ReadAsStringAsync().Result;\\n}\\n</code></pre>\\n\\n<p>How do I read <code>jsonContent</code> based on a key value pair passed by the Javascript FormData? </p>\\n\\n<p>I tried to do <code>JsonConvert.DeserializeObject&lt;?&gt;</code>, but it requires a particular type to deserialize into.</p>\\n\\n<p>I want to get the value of the key <code>\"filesToDelete\"</code> passed from the Javascript FormData.</p>\\n\\n<p>How can I get this value?</p>', label=0)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_stackoverflow.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explore and evaluate data\n",
    "\n",
    "At this point, we assume that data has been properly checked while model was created.\n",
    "\n",
    "We leave it now but proper checking is warranted. Just do a plot of transactions over time.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "[UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `labellll` cannot be resolved. Did you mean one of the following? [`label`, `count`].;\n'Sort ['labellll ASC NULLS FIRST], true\n+- Aggregate [label#2L], [label#2L, count(1) AS count#32L]\n   +- Relation [title#0,body#1,label#2L] parquet\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 6\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Just to remember ... [`body`, `label`, `title`]\u001b[39;00m\n\u001b[1;32m      3\u001b[0m df_plot \u001b[38;5;241m=\u001b[39m ( df_stackoverflow\n\u001b[1;32m      4\u001b[0m             \u001b[38;5;241m.\u001b[39mgroupby([\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m      5\u001b[0m             \u001b[38;5;241m.\u001b[39mcount()\n\u001b[0;32m----> 6\u001b[0m             \u001b[38;5;241m.\u001b[39msort(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabellll\u001b[39m\u001b[38;5;124m'\u001b[39m, ascending\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m      7\u001b[0m             \u001b[38;5;241m.\u001b[39mtoPandas()\n\u001b[1;32m      8\u001b[0m           )\n\u001b[1;32m      9\u001b[0m plotBarColoured(df_plot, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtitle\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcount\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124morange\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     10\u001b[0m plt\u001b[38;5;241m.\u001b[39mtitle(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNumber of ...\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/pyspark_env/lib/python3.11/site-packages/pyspark/sql/dataframe.py:2736\u001b[0m, in \u001b[0;36mDataFrame.sort\u001b[0;34m(self, *cols, **kwargs)\u001b[0m\n\u001b[1;32m   2645\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msort\u001b[39m(\n\u001b[1;32m   2646\u001b[0m     \u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39mcols: Union[\u001b[38;5;28mstr\u001b[39m, Column, List[Union[\u001b[38;5;28mstr\u001b[39m, Column]]], \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any\n\u001b[1;32m   2647\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataFrame\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m   2648\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Returns a new :class:`DataFrame` sorted by the specified column(s).\u001b[39;00m\n\u001b[1;32m   2649\u001b[0m \n\u001b[1;32m   2650\u001b[0m \u001b[38;5;124;03m    .. versionadded:: 1.3.0\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2734\u001b[0m \u001b[38;5;124;03m    +---+-----+\u001b[39;00m\n\u001b[1;32m   2735\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 2736\u001b[0m     jdf \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jdf\u001b[38;5;241m.\u001b[39msort(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sort_cols(cols, kwargs))\n\u001b[1;32m   2737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m DataFrame(jdf, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msparkSession)\n",
      "File \u001b[0;32m~/anaconda3/envs/pyspark_env/lib/python3.11/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m get_return_value(\n\u001b[1;32m   1323\u001b[0m     answer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_id, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname)\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m~/anaconda3/envs/pyspark_env/lib/python3.11/site-packages/pyspark/errors/exceptions/captured.py:185\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    181\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    184\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 185\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `labellll` cannot be resolved. Did you mean one of the following? [`label`, `count`].;\n'Sort ['labellll ASC NULLS FIRST], true\n+- Aggregate [label#2L], [label#2L, count(1) AS count#32L]\n   +- Relation [title#0,body#1,label#2L] parquet\n"
     ]
    }
   ],
   "source": [
    "# Just to remember ... [`body`, `label`, `title`]\n",
    "\n",
    "df_plot = ( df_stackoverflow\n",
    "            .groupby(['label'])\n",
    "            .count()\n",
    "            .sort('labellll', ascending=True)\n",
    "            .toPandas()\n",
    "          )\n",
    "plotBarColoured(df_plot, 'title', 'count', 'orange')\n",
    "plt.title('Number of ...')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_plot.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Stream\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "As we have no real time scenario in place, we will simulate a data stream by creating a built-in `rate` source to generate events at 1-second intervals, and join those 'ticks' with data from our downloaded dataset. This creates a regular stream of sample values. \n",
    "\n",
    "Alternatively, for pratical applications, we could have used an Apache Kafka source or even a file source.\n",
    "Apache Kafka was the best solution for that matter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "rate_source = spark.readStream.format(\"rate\").load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- value: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rate_source.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We should guarantee that we are using data sorted by transaction id (aka time)\n",
    "\n",
    "df_transactions = df_transactions.sort('Transaction_ID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 13:======================================>                   (6 + 3) / 9]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+---+-------------------+----+---+--------------+\n",
      "|Year|Month|Day|Time               |Hour|Min|Transaction_ID|\n",
      "+----+-----+---+-------------------+----+---+--------------+\n",
      "|2002|9    |1  |2023-05-02 06:21:00|6   |21 |0             |\n",
      "|2002|9    |1  |2023-05-02 06:42:00|6   |42 |1             |\n",
      "|2002|9    |2  |2023-05-02 06:22:00|6   |22 |2             |\n",
      "|2002|9    |2  |2023-05-02 17:45:00|17  |45 |3             |\n",
      "|2002|9    |3  |2023-05-02 06:23:00|6   |23 |4             |\n",
      "|2002|9    |3  |2023-05-02 13:53:00|13  |53 |5             |\n",
      "|2002|9    |4  |2023-05-02 05:51:00|5   |51 |6             |\n",
      "|2002|9    |4  |2023-05-02 06:09:00|6   |9  |7             |\n",
      "|2002|9    |5  |2023-05-02 06:14:00|6   |14 |8             |\n",
      "|2002|9    |5  |2023-05-02 09:35:00|9   |35 |9             |\n",
      "|2002|9    |5  |2023-05-02 20:18:00|20  |18 |10            |\n",
      "|2002|9    |5  |2023-05-02 20:41:00|20  |41 |11            |\n",
      "|2002|9    |6  |2023-05-02 06:16:00|6   |16 |12            |\n",
      "|2002|9    |7  |2023-05-02 06:16:00|6   |16 |13            |\n",
      "|2002|9    |7  |2023-05-02 06:34:00|6   |34 |14            |\n",
      "|2002|9    |7  |2023-05-02 09:39:00|9   |39 |15            |\n",
      "|2002|9    |8  |2023-05-02 06:10:00|6   |10 |16            |\n",
      "|2002|9    |8  |2023-05-02 06:38:00|6   |38 |17            |\n",
      "|2002|9    |8  |2023-05-02 13:48:00|13  |48 |18            |\n",
      "|2002|9    |8  |2023-05-02 22:01:00|22  |1  |19            |\n",
      "+----+-----+---+-------------------+----+---+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_transactions.select('Year', 'Month', 'Day', 'Time', 'Hour', 'Min', 'Transaction_ID').show(20,truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a continuous data stream\n",
    "\n",
    "Circularly replaying the data as long as the process is running. It will be a simulated streaming version of the data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Circularly replay the data\n",
    "\n",
    "data_stream = ( rate_source\n",
    "                   .select(F.expr(f'value % {num_transactions}').alias('Transaction_Id'), 'timestamp')\n",
    "                   .join(df_transactions, 'Transaction_Id')\n",
    "              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Transaction_Id: long (nullable = true)\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- User: integer (nullable = true)\n",
      " |-- Card: integer (nullable = true)\n",
      " |-- Year: integer (nullable = true)\n",
      " |-- Month: integer (nullable = true)\n",
      " |-- Day: integer (nullable = true)\n",
      " |-- Time: timestamp (nullable = true)\n",
      " |-- Use Chip: string (nullable = true)\n",
      " |-- Merchant Name: long (nullable = true)\n",
      " |-- Merchant City: string (nullable = true)\n",
      " |-- Merchant State: string (nullable = true)\n",
      " |-- Zip: double (nullable = true)\n",
      " |-- MCC: integer (nullable = true)\n",
      " |-- Is Fraud?: string (nullable = true)\n",
      " |-- Correct Amount: float (nullable = true)\n",
      " |-- Hour: integer (nullable = true)\n",
      " |-- Min: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_stream.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_check = data_stream.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model deployment\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading the binary classification model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Read the ML model via pipeline api (not the simple pipeline)\n",
    "\n",
    "persisted_model = PipelineModel.load(data_dir+'model-LinearSVM-credit-cards')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[StringIndexerModel: uid=StringIndexer_e1064637f5a0, handleInvalid=skip, numInputCols=2, numOutputCols=2,\n",
       " OneHotEncoderModel: uid=OneHotEncoder_6a398697f374, dropLast=true, handleInvalid=error, numInputCols=2, numOutputCols=2,\n",
       " VectorAssembler_f41b5139a541,\n",
       " LinearSVCModel: uid=LinearSVC_912312635f03, numClasses=2, numFeatures=11838]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the model, namely the stages that were used\n",
    "\n",
    "persisted_model.stages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Streaming data transformer\n",
    "\n",
    "Let us set the operation to be applied to the stream.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ML model directly applied to the streaming dataframe using `transform`\n",
    "\n",
    "prediction_stream = persisted_model.transform(data_stream)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Transaction_Id: long (nullable = true)\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- User: integer (nullable = true)\n",
      " |-- Card: integer (nullable = true)\n",
      " |-- Year: integer (nullable = true)\n",
      " |-- Month: integer (nullable = true)\n",
      " |-- Day: integer (nullable = true)\n",
      " |-- Time: timestamp (nullable = true)\n",
      " |-- Use Chip: string (nullable = true)\n",
      " |-- Merchant Name: long (nullable = true)\n",
      " |-- Merchant City: string (nullable = true)\n",
      " |-- Merchant State: string (nullable = true)\n",
      " |-- Zip: double (nullable = true)\n",
      " |-- MCC: integer (nullable = true)\n",
      " |-- Is Fraud?: string (nullable = true)\n",
      " |-- Correct Amount: float (nullable = true)\n",
      " |-- Hour: integer (nullable = true)\n",
      " |-- Min: integer (nullable = true)\n",
      " |-- Use Chip Index: double (nullable = false)\n",
      " |-- Merchant City Index: double (nullable = false)\n",
      " |-- Use Chip OHE: vector (nullable = true)\n",
      " |-- Merchant City OHE: vector (nullable = true)\n",
      " |-- features: vector (nullable = true)\n",
      " |-- rawPrediction: vector (nullable = true)\n",
      " |-- prediction: double (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prediction_stream.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Consuming predictions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final step is to do something with the prediction data.\n",
    "For the time being, we are going to limit this step to just querying the data.\n",
    "\n",
    "For real-world application, we can offer this kind of service to other applications.\n",
    "\n",
    "Maybe in the form of an HTTP-based API or through pub/sub messaging interactions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Transaction_Id',\n",
       " 'timestamp',\n",
       " 'User',\n",
       " 'Card',\n",
       " 'Year',\n",
       " 'Month',\n",
       " 'Day',\n",
       " 'Time',\n",
       " 'Use Chip',\n",
       " 'Merchant Name',\n",
       " 'Merchant City',\n",
       " 'Merchant State',\n",
       " 'Zip',\n",
       " 'MCC',\n",
       " 'Is Fraud?',\n",
       " 'Correct Amount',\n",
       " 'Hour',\n",
       " 'Min',\n",
       " 'prediction']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cols_to_check.append('prediction')\n",
    "cols_to_check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Just in case we want to start a table containing results but from scratch\n",
    "\n",
    "spark.sql(\"drop table if exists cardtransactionstable\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/04/11 20:39:32 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /private/var/folders/cw/wl2n47pd62b20l16hsnf0h1c0000gn/T/temporary-3d209751-031d-4f75-b465-1a5bf18d180a. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "24/04/11 20:39:32 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 31:>                                                         (0 + 8) / 9]\r"
     ]
    }
   ],
   "source": [
    "# In case we want to store in an in-memory table (the sink). \n",
    "# The query name will be the table name\n",
    "\n",
    "# After executing the code, the streaming computation will start in the background\n",
    "\n",
    "query_1 = ( prediction_stream\n",
    "                        .select(cols_to_check)\n",
    "                        .writeStream\n",
    "                        .queryName(\"cardtransactionstable\")\n",
    "                        .outputMode(\"append\")  # append, update\n",
    "                        .format(\"memory\")\n",
    "                        .start()\n",
    "                   )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup an aggregation by day concerning the number of frauds detected\n",
    "#\n",
    "# We leave this as exercise\n",
    "\n",
    "# fraud_count = ...\n",
    "\n",
    "# query_2 = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Some extra checks\n",
    "\n",
    "spark.streams.active[0].isActive\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'message': 'Processing new data',\n",
       " 'isDataAvailable': True,\n",
       " 'isTriggerActive': True}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/04/11 20:39:49 WARN DAGScheduler: Broadcasting large task binary with size 1498.0 KiB\n"
     ]
    }
   ],
   "source": [
    "query_1.status\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 33:>                 (0 + 8) / 9][Stage 34:>                 (0 + 0) / 8]\r"
     ]
    }
   ],
   "source": [
    "query_1.lastProgress\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------------------+-----------+\n",
      "|namespace|tableName            |isTemporary|\n",
      "+---------+---------------------+-----------+\n",
      "|         |cardtransactionstable|true       |\n",
      "+---------+---------------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Figure out the tables we hold\n",
    "\n",
    "spark.sql(\"show tables\").show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 36:>   (1 + 8) / 9][Stage 37:>   (0 + 0) / 8][Stage 39:>   (0 + 0) / 1]8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-----------------------+----+----+----+-----+---+-------------------+------------------+--------------------+-------------+--------------+-------+----+---------+--------------+----+---+----------+\n",
      "|Transaction_Id|timestamp              |User|Card|Year|Month|Day|Time               |Use Chip          |Merchant Name       |Merchant City|Merchant State|Zip    |MCC |Is Fraud?|Correct Amount|Hour|Min|prediction|\n",
      "+--------------+-----------------------+----+----+----+-----+---+-------------------+------------------+--------------------+-------------+--------------+-------+----+---------+--------------+----+---+----------+\n",
      "|16            |2024-04-11 20:39:48.401|0   |0   |2002|9    |8  |2023-05-02 06:10:00|Swipe Transaction |-34551508091458520  |La Verne     |CA            |91750.0|5912|No       |147.45        |6   |10 |0.0       |\n",
      "|0             |2024-04-11 20:39:32.401|0   |0   |2002|9    |1  |2023-05-02 06:21:00|Swipe Transaction |3527213246127876953 |La Verne     |CA            |91750.0|5300|No       |134.09        |6   |21 |0.0       |\n",
      "|13            |2024-04-11 20:39:45.401|0   |0   |2002|9    |7  |2023-05-02 06:16:00|Swipe Transaction |-727612092139916043 |Monterey Park|CA            |91754.0|5411|No       |117.05        |6   |16 |0.0       |\n",
      "|2             |2024-04-11 20:39:34.401|0   |0   |2002|9    |2  |2023-05-02 06:22:00|Swipe Transaction |-727612092139916043 |Monterey Park|CA            |91754.0|5411|No       |120.34        |6   |22 |0.0       |\n",
      "|4             |2024-04-11 20:39:36.401|0   |0   |2002|9    |3  |2023-05-02 06:23:00|Swipe Transaction |5817218446178736267 |La Verne     |CA            |91750.0|5912|No       |104.71        |6   |23 |0.0       |\n",
      "|5             |2024-04-11 20:39:37.401|0   |0   |2002|9    |3  |2023-05-02 13:53:00|Swipe Transaction |-7146670748125200898|Monterey Park|CA            |91755.0|5970|No       |86.19         |13  |53 |0.0       |\n",
      "|8             |2024-04-11 20:39:40.401|0   |0   |2002|9    |5  |2023-05-02 06:14:00|Swipe Transaction |-727612092139916043 |Monterey Park|CA            |91754.0|5411|No       |61.72         |6   |14 |0.0       |\n",
      "|15            |2024-04-11 20:39:47.401|0   |0   |2002|9    |7  |2023-05-02 09:39:00|Swipe Transaction |4055257078481058705 |La Verne     |CA            |91750.0|7538|No       |29.34         |9   |39 |0.0       |\n",
      "|3             |2024-04-11 20:39:35.401|0   |0   |2002|9    |2  |2023-05-02 17:45:00|Swipe Transaction |3414527459579106770 |Monterey Park|CA            |91754.0|5651|No       |128.95        |17  |45 |0.0       |\n",
      "|6             |2024-04-11 20:39:38.401|0   |0   |2002|9    |4  |2023-05-02 05:51:00|Swipe Transaction |-727612092139916043 |Monterey Park|CA            |91754.0|5411|No       |93.84         |5   |51 |0.0       |\n",
      "|7             |2024-04-11 20:39:39.401|0   |0   |2002|9    |4  |2023-05-02 06:09:00|Swipe Transaction |-727612092139916043 |Monterey Park|CA            |91754.0|5411|No       |123.5         |6   |9  |0.0       |\n",
      "|11            |2024-04-11 20:39:43.401|0   |0   |2002|9    |5  |2023-05-02 20:41:00|Online Transaction|-9092677072201095172|ONLINE       |NULL          |NULL   |4900|No       |53.91         |20  |41 |0.0       |\n",
      "|12            |2024-04-11 20:39:44.401|0   |0   |2002|9    |6  |2023-05-02 06:16:00|Swipe Transaction |2027553650310142703 |Mira Loma    |CA            |91752.0|5541|No       |110.37        |6   |16 |0.0       |\n",
      "|14            |2024-04-11 20:39:46.401|0   |0   |2002|9    |7  |2023-05-02 06:34:00|Swipe Transaction |-5475680618560174533|Monterey Park|CA            |91755.0|5942|No       |45.3          |6   |34 |0.0       |\n",
      "|1             |2024-04-11 20:39:33.401|0   |0   |2002|9    |1  |2023-05-02 06:42:00|Swipe Transaction |-727612092139916043 |Monterey Park|CA            |91754.0|5411|No       |38.48         |6   |42 |0.0       |\n",
      "|9             |2024-04-11 20:39:41.401|0   |0   |2002|9    |5  |2023-05-02 09:35:00|Swipe Transaction |4055257078481058705 |La Verne     |CA            |91750.0|7538|No       |57.1          |9   |35 |0.0       |\n",
      "|10            |2024-04-11 20:39:42.401|0   |0   |2002|9    |5  |2023-05-02 20:18:00|Swipe Transaction |-4500542936415012428|La Verne     |CA            |91750.0|5814|No       |76.07         |20  |18 |0.0       |\n",
      "+--------------+-----------------------+----+----+----+-----+---+-------------------+------------------+--------------------+-------------+--------------+-------+----+---------+--------------+----+---+----------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/04/11 20:40:25 WARN DAGScheduler: Broadcasting large task binary with size 1498.6 KiB\n",
      "[Stage 38:>                                                         (0 + 6) / 6]\r"
     ]
    }
   ],
   "source": [
    "# Interactively query in-memory table\n",
    "\n",
    "spark.sql(\"select * from cardtransactionstable\").show(truncate=False)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "|      17|\n",
      "+--------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 45:>                 (0 + 8) / 9][Stage 46:>                 (0 + 0) / 8]\r"
     ]
    }
   ],
   "source": [
    "# Interactively another query in-memory table\n",
    "\n",
    "spark.sql(\"select count(*) from cardtransactionstable\").show()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\n",
    "Visual analysis ... we leave it as an exercise!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stopping the process\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/04/11 20:40:38 ERROR WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 3, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@51bfef7f] is aborting.\n",
      "24/04/11 20:40:38 ERROR WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 3, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@51bfef7f] aborted.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/04/11 20:40:38 WARN TaskSetManager: Lost task 1.0 in stage 45.0 (TID 128) (10.192.11.24 executor driver): TaskKilled (Stage cancelled: Job 33 cancelled part of cancelled job group 52d5ebed-088d-40e7-b7d3-3390f80ea61c)\n",
      "24/04/11 20:40:38 WARN TaskSetManager: Lost task 5.0 in stage 45.0 (TID 132) (10.192.11.24 executor driver): TaskKilled (Stage cancelled: Job 33 cancelled part of cancelled job group 52d5ebed-088d-40e7-b7d3-3390f80ea61c)\n",
      "24/04/11 20:40:38 WARN TaskSetManager: Lost task 7.0 in stage 45.0 (TID 134) (10.192.11.24 executor driver): TaskKilled (Stage cancelled: Job 33 cancelled part of cancelled job group 52d5ebed-088d-40e7-b7d3-3390f80ea61c)\n",
      "24/04/11 20:40:38 WARN TaskSetManager: Lost task 4.0 in stage 45.0 (TID 131) (10.192.11.24 executor driver): TaskKilled (Stage cancelled: Job 33 cancelled part of cancelled job group 52d5ebed-088d-40e7-b7d3-3390f80ea61c)\n",
      "24/04/11 20:40:38 WARN TaskSetManager: Lost task 0.0 in stage 45.0 (TID 127) (10.192.11.24 executor driver): TaskKilled (Stage cancelled: Job 33 cancelled part of cancelled job group 52d5ebed-088d-40e7-b7d3-3390f80ea61c)\n",
      "24/04/11 20:40:38 WARN TaskSetManager: Lost task 3.0 in stage 45.0 (TID 130) (10.192.11.24 executor driver): TaskKilled (Stage cancelled: Job 33 cancelled part of cancelled job group 52d5ebed-088d-40e7-b7d3-3390f80ea61c)\n",
      "24/04/11 20:40:38 WARN TaskSetManager: Lost task 6.0 in stage 45.0 (TID 133) (10.192.11.24 executor driver): TaskKilled (Stage cancelled: Job 33 cancelled part of cancelled job group 52d5ebed-088d-40e7-b7d3-3390f80ea61c)\n",
      "24/04/11 20:40:38 WARN TaskSetManager: Lost task 2.0 in stage 45.0 (TID 129) (10.192.11.24 executor driver): TaskKilled (Stage cancelled: Job 33 cancelled part of cancelled job group 52d5ebed-088d-40e7-b7d3-3390f80ea61c)\n"
     ]
    }
   ],
   "source": [
    "# We can turn off the query now and eventually set up a different one\n",
    "\n",
    "query_1.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 30:===================================================>      (8 + 1) / 9]\r"
     ]
    }
   ],
   "source": [
    "# Notice that in a production environment, we have to establish \n",
    "# that the query is awaiting termination so to prevent the driver \n",
    "# process from termination when the stream is ative\n",
    "\n",
    "# query_1.awaitTermination()\n",
    "\n",
    "# query_2.awaitTermination()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Additional exercise\n",
    "\n",
    "Once this exercise is completed, create a new notebook with similar implementation but using a different streaming setup. Specifically, relying also in the messaging system Apache Kafka.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# References\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "- Learning Spark - Lightning-Fast Data Analytics, 2nd Ed. J. Damji, B. Wenig, T. Das, and D. Lee. O'Reilly, 2020\n",
    "- Stream Processing with Apache Spark. G. Maas and F. Garillot. O'Reilly, 2019\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyspark_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "249px",
    "width": "332px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "204.98px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
